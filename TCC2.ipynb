{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a75f0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\linco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\linco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import dl_translate as dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb363ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "  \n",
    "tokenizerR = AutoTokenizer.from_pretrained(\"rabindralamsal/finetuned-bertweet-sentiment-analysis\")\n",
    "\n",
    "modelR = TFAutoModelForSequenceClassification.from_pretrained(\"rabindralamsal/finetuned-bertweet-sentiment-analysis\", from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e150b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999840259552002}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I loved Star Wars so much!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a60eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-pt-en-t5\")\n",
    "pten_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
    "mt = dlt.TranslationModel()  # Slow when you load it for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adff6713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97267216 0.02368472 0.00364307]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "example_tweet = \"The NEET exams show our Govt in a poor light: unresponsiveness to genuine concerns; admit cards not delivered to aspirants in time; failure to provide centres in towns they reside, thus requiring unnecessary & risky travels. What a disgrace to treat our #Covid warriors like this!\"\n",
    "#this tweet resides on Twitter with an identifier-1435793872588738560\n",
    "    \n",
    "input = tokenizerR.encode(example_tweet, return_tensors=\"tf\")\n",
    "output = modelR.predict(input)[0]\n",
    "prediction = tf.nn.softmax(output, axis=1).numpy()\n",
    "sentiment = np.argmax(prediction)\n",
    "    \n",
    "print(prediction)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc8b5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portuguese = pd.read_csv(\"datasets/export_TweetSentBR.csv\")\n",
    "df_english = pd.read_csv(\"datasets/export_TweetSentEnglish.csv\")\n",
    "dataset = pd.read_csv(\"./datasetcompleto.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "861c2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove = df_portuguese[df_portuguese['sentiment'] == '-']\n",
    "df_portuguese = df_portuguese.drop(df_remove.index)\n",
    "df_portuguese = df_portuguese.reset_index()\n",
    "df_portuguese = df_portuguese.drop(columns=['index'])\n",
    "df_portuguese['sentiment'] = df_portuguese['sentiment'].apply(lambda x: int(x))\n",
    "Tweet = df_portuguese['text']\n",
    "polarity = np.asarray(df_portuguese['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06dcf47",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUnnamed: 0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dataset\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py:4906\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4774\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   4775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4783\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4784\u001b[0m ):\n\u001b[0;32m   4785\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4787\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4904\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4905\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4908\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\generic.py:4150\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4150\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\generic.py:4185\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4184\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4185\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4186\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{axis_name: new_axis})\n\u001b[0;32m   4188\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6017\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6017\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6018\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6019\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "dataset.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13699adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets_in_English = pd.DataFrame( data = {'phrases': Tweets_in_Portuguese['frases'].apply(translater_phrases), 'classification': polarity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "921275ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets_in_Portuguese = pd.DataFrame( data = {'frases': Tweet ,'classificacao' : polarity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f6db168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portuguese['text'] = df_portuguese['text'].apply(remove_user)\n",
    "df_portuguese.drop(columns = ['id', 'id_twitter'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce59ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f598c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrases</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The #encontro Program was showing a family tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I loved them.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CATRA launching its new song PPK CHORA no k</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#MasterChefBR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm handled with this guy.... how mu\"</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11528</th>\n",
       "      <td>I'm already here ready for #MasterChefBR but\"</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11529</th>\n",
       "      <td>One thing I don't have the courage is this\"</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11530</th>\n",
       "      <td>#By You are beautiful #ByYou are beauti</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11531</th>\n",
       "      <td>What pride of you,! #I meet you</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11532</th>\n",
       "      <td>People look at the biceps of this priest #Conv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11533 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 phrases  classification\n",
       "0      The #encontro Program was showing a family tha...               1\n",
       "1                                          I loved them.               1\n",
       "2            CATRA launching its new song PPK CHORA no k               1\n",
       "3                                          #MasterChefBR               0\n",
       "4                  I'm handled with this guy.... how mu\"              -1\n",
       "...                                                  ...             ...\n",
       "11528      I'm already here ready for #MasterChefBR but\"              -1\n",
       "11529        One thing I don't have the courage is this\"              -1\n",
       "11530            #By You are beautiful #ByYou are beauti               1\n",
       "11531                    What pride of you,! #I meet you               1\n",
       "11532  People look at the biceps of this priest #Conv...               1\n",
       "\n",
       "[11533 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b91a177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Que coisa linda! O Programa #encontro estava m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por mais #Encontro com as Irm√£s Galv√£o, adorei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. CATRA  lan√ßando sua nova m√∫sica PPK CHORA ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quem viu aquela lutadora modela barbuda tatuad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T√¥ passada com esse cara.... quanta merda pode...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11565</th>\n",
       "      <td>eu ja to aqui pronto pro #MasterChefBR mas ain...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11566</th>\n",
       "      <td>MALUCO! Uma coisa que eu n√£o tenho coragem √© e...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11567</th>\n",
       "      <td>#MaisVoce   est√° linda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11568</th>\n",
       "      <td>Que orgulho de ti, ! #Encontro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11569</th>\n",
       "      <td>Gente olha o b√≠ceps desse padre #ConversaComBial</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11570 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0      Que coisa linda! O Programa #encontro estava m...         1\n",
       "1      Por mais #Encontro com as Irm√£s Galv√£o, adorei...         1\n",
       "2      Mr. CATRA  lan√ßando sua nova m√∫sica PPK CHORA ...         1\n",
       "3      quem viu aquela lutadora modela barbuda tatuad...         0\n",
       "4      T√¥ passada com esse cara.... quanta merda pode...        -1\n",
       "...                                                  ...       ...\n",
       "11565  eu ja to aqui pronto pro #MasterChefBR mas ain...        -1\n",
       "11566  MALUCO! Uma coisa que eu n√£o tenho coragem √© e...        -1\n",
       "11567                            #MaisVoce   est√° linda          1\n",
       "11568                     Que orgulho de ti, ! #Encontro         1\n",
       "11569   Gente olha o b√≠ceps desse padre #ConversaComBial         1\n",
       "\n",
       "[11570 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a2bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_X(frases):\n",
    "    lista = []\n",
    "    \n",
    "    for frase in frases:\n",
    "        lista.append(frase)\n",
    "        \n",
    "    return lista\n",
    "\n",
    "def pre_Y(number):\n",
    "    lista = []\n",
    "    \n",
    "    for numb in number:\n",
    "        lista.append(numb)\n",
    "    \n",
    "    return lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "719331ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_array(frases):\n",
    "    \n",
    "    vocab = []\n",
    "    palavras = []\n",
    "    for frase in frases:\n",
    "        \n",
    "        text_array = remove_user(frase)\n",
    "        text_array = Tokenize(text_array)\n",
    "        text_array = text_array.split(' ')\n",
    "        for i in range(len(text_array)):\n",
    "            vocab.append(text_array[i])\n",
    "    \n",
    "    \n",
    "        \n",
    "    return vocab\n",
    "\n",
    "def Tokenize(f):     ## Pre-processando a frase\n",
    "    \n",
    "    ## Colocando em minusculo\n",
    "    ## Retirando a pontua√ßao\n",
    "    ## Retirando as StopWords\n",
    "    \n",
    "    f = f.lower().replace('\\n', '').replace('-','').replace('#','').replace('.','').replace(',','').replace('!','').replace('r\\n','').replace('  ','')\n",
    "    token = RegexpTokenizer(r\"\\w+\")\n",
    "    f = token.tokenize(f)\n",
    "    \n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    \n",
    "    new_word = [word for word in f if not word in stop_words]\n",
    "    \n",
    "    return ' '.join(new_word)\n",
    "\n",
    "def remove_user(frase):\n",
    "\n",
    "    return re.sub('@\\w+','',frase)\n",
    "\n",
    "def translater_phrases(frase):\n",
    "    translater = pten_pipeline(frase)\n",
    "    translater = str(translater).strip('[{}]')\n",
    "    translater = translater[19:]\n",
    "    translater = translater.strip(\"''\")\n",
    "    \n",
    "    return translater\n",
    "\n",
    "def translater_frases(frase):\n",
    "    text_hi = frase\n",
    "    translater = mt.translate(text_hi, source=dlt.lang.PORTUGUESE, target=dlt.lang.ENGLISH)\n",
    "    \n",
    "    return translater\n",
    "\n",
    "def new_classifier(frase):\n",
    "    input = tokenizerR.encode(frase, return_tensors=\"tf\")\n",
    "    output = modelR.predict(input)[0]\n",
    "    prediction = tf.nn.softmax(output, axis=1).numpy()\n",
    "    sentiment = np.argmax(prediction)\n",
    "    \n",
    "    return sentiment \n",
    "\n",
    "def classifierD(frase):    \n",
    "    return classifier(frase) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b0fb0bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Tweet \u001b[38;5;241m=\u001b[39m \u001b[43mTweet\u001b[49m\u001b[38;5;241m.\u001b[39mapply(remove_user)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tweet' is not defined"
     ]
    }
   ],
   "source": [
    "Tweet = Tweet.apply(remove_user)\n",
    "# Tweet_preprocessed = Tweet.apply(Tokenize)\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train = count_vect.fit_transform(Tweet_preprocessed)\n",
    "\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_train_transform = tfidf_transformer.fit_transform(X_train) # Aplicando o TF-IDF\n",
    "\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_train_transform, polarity, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1ef5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_in_English = pd.DataFrame(data = {'tweets_original': df_portuguese['text'][:500], 'tweets_translater_unicamp': df_english['phrases'][:500],'tweets_translater_face': df_portuguese['text'][:500].apply(translater_frases)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8aec7626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_original</th>\n",
       "      <th>tweets_translater_unicamp</th>\n",
       "      <th>tweets_translater_face</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Que coisa linda! O Programa #encontro estava m...</td>\n",
       "      <td>The #encontro Program was showing a family tha...</td>\n",
       "      <td>What a beautiful thing! The #touch program was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por mais #Encontro com as Irm√£s Galv√£o, adorei...</td>\n",
       "      <td>I loved them.</td>\n",
       "      <td>For more I met with the Galvan Sisters, I love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. CATRA  lan√ßando sua nova m√∫sica PPK CHORA ...</td>\n",
       "      <td>CATRA launching its new song PPK CHORA no k</td>\n",
       "      <td>Mr. CATRA releases his new song PPK CHORA on k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quem viu aquela lutadora modela barbuda tatuad...</td>\n",
       "      <td>#MasterChefBR</td>\n",
       "      <td>Who saw that fighter model tattooed? #MasterCh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T√¥ passada com esse cara.... quanta merda pode...</td>\n",
       "      <td>I'm handled with this guy.... how mu\"</td>\n",
       "      <td>How much shit can come out of someone‚Äôs mouth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>#VideoShowAoVivo Kkkk ahhh eu morro com esse V...</td>\n",
       "      <td>I love this guy!</td>\n",
       "      <td>#VideoShowAoLive Kkkk ahhh I die with this Sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>At√© o Padre F√°bio de Melo cantando Trem Bala, ...</td>\n",
       "      <td>Even Father F√°bio de Melo singing Trem Bala, i...</td>\n",
       "      <td>Even the Fabius Father of Melo singing Trem Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>#MaisVoc√™ minha irm√£ cal√ßa 34 e compra sapato ...</td>\n",
       "      <td>Never had difficulty in finding this number of...</td>\n",
       "      <td>You my sister shoes 34 and buy shoes in any st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Iris t√£o fofa e linda üòªüòªüíô #EDeCasa</td>\n",
       "      <td>Iris so fool and beautiful Iris  #EDeCasa</td>\n",
       "      <td>Iris so foolish and beautiful üèª #EDeCasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>#MaisVoce O FGTS n√£o √© descontado do funcion√°r...</td>\n",
       "      <td>The obligation is only of the company, which must</td>\n",
       "      <td>#MaisVoce The FGTS is not discounted from the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweets_original  \\\n",
       "0    Que coisa linda! O Programa #encontro estava m...   \n",
       "1    Por mais #Encontro com as Irm√£s Galv√£o, adorei...   \n",
       "2    Mr. CATRA  lan√ßando sua nova m√∫sica PPK CHORA ...   \n",
       "3    quem viu aquela lutadora modela barbuda tatuad...   \n",
       "4    T√¥ passada com esse cara.... quanta merda pode...   \n",
       "..                                                 ...   \n",
       "495  #VideoShowAoVivo Kkkk ahhh eu morro com esse V...   \n",
       "496  At√© o Padre F√°bio de Melo cantando Trem Bala, ...   \n",
       "497  #MaisVoc√™ minha irm√£ cal√ßa 34 e compra sapato ...   \n",
       "498                 Iris t√£o fofa e linda üòªüòªüíô #EDeCasa   \n",
       "499  #MaisVoce O FGTS n√£o √© descontado do funcion√°r...   \n",
       "\n",
       "                             tweets_translater_unicamp  \\\n",
       "0    The #encontro Program was showing a family tha...   \n",
       "1                                        I loved them.   \n",
       "2          CATRA launching its new song PPK CHORA no k   \n",
       "3                                        #MasterChefBR   \n",
       "4                I'm handled with this guy.... how mu\"   \n",
       "..                                                 ...   \n",
       "495                                   I love this guy!   \n",
       "496  Even Father F√°bio de Melo singing Trem Bala, i...   \n",
       "497  Never had difficulty in finding this number of...   \n",
       "498          Iris so fool and beautiful Iris  #EDeCasa   \n",
       "499  The obligation is only of the company, which must   \n",
       "\n",
       "                                tweets_translater_face  \n",
       "0    What a beautiful thing! The #touch program was...  \n",
       "1    For more I met with the Galvan Sisters, I love...  \n",
       "2    Mr. CATRA releases his new song PPK CHORA on k...  \n",
       "3    Who saw that fighter model tattooed? #MasterCh...  \n",
       "4    How much shit can come out of someone‚Äôs mouth ...  \n",
       "..                                                 ...  \n",
       "495  #VideoShowAoLive Kkkk ahhh I die with this Sum...  \n",
       "496  Even the Fabius Father of Melo singing Trem Ba...  \n",
       "497  You my sister shoes 34 and buy shoes in any st...  \n",
       "498           Iris so foolish and beautiful üèª #EDeCasa  \n",
       "499  #MaisVoce The FGTS is not discounted from the ...  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets_in_English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "500cd821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_original</th>\n",
       "      <th>tweets_translater_unicamp</th>\n",
       "      <th>tweets_translater_face</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Que coisa linda! O Programa #encontro estava m...</td>\n",
       "      <td>The #encontro Program was showing a family tha...</td>\n",
       "      <td>What a beautiful thing! The #touch program was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por mais #Encontro com as Irm√£s Galv√£o, adorei...</td>\n",
       "      <td>I loved them.</td>\n",
       "      <td>For more I met with the Galvan Sisters, I love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. CATRA  lan√ßando sua nova m√∫sica PPK CHORA ...</td>\n",
       "      <td>CATRA launching its new song PPK CHORA no k</td>\n",
       "      <td>Mr. CATRA releases his new song PPK CHORA on k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quem viu aquela lutadora modela barbuda tatuad...</td>\n",
       "      <td>#MasterChefBR</td>\n",
       "      <td>Who saw that fighter model tattooed? #MasterCh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T√¥ passada com esse cara.... quanta merda pode...</td>\n",
       "      <td>I'm handled with this guy.... how mu\"</td>\n",
       "      <td>How much shit can come out of someone‚Äôs mouth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>#VideoShowAoVivo Kkkk ahhh eu morro com esse V...</td>\n",
       "      <td>I love this guy!</td>\n",
       "      <td>#VideoShowAoLive Kkkk ahhh I die with this Sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>At√© o Padre F√°bio de Melo cantando Trem Bala, ...</td>\n",
       "      <td>Even Father F√°bio de Melo singing Trem Bala, i...</td>\n",
       "      <td>Even the Fabius Father of Melo singing Trem Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>#MaisVoc√™ minha irm√£ cal√ßa 34 e compra sapato ...</td>\n",
       "      <td>Never had difficulty in finding this number of...</td>\n",
       "      <td>You my sister shoes 34 and buy shoes in any st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Iris t√£o fofa e linda üòªüòªüíô #EDeCasa</td>\n",
       "      <td>Iris so fool and beautiful Iris  #EDeCasa</td>\n",
       "      <td>Iris so foolish and beautiful üèª #EDeCasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>#MaisVoce O FGTS n√£o √© descontado do funcion√°r...</td>\n",
       "      <td>The obligation is only of the company, which must</td>\n",
       "      <td>#MaisVoce The FGTS is not discounted from the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweets_original  \\\n",
       "0    Que coisa linda! O Programa #encontro estava m...   \n",
       "1    Por mais #Encontro com as Irm√£s Galv√£o, adorei...   \n",
       "2    Mr. CATRA  lan√ßando sua nova m√∫sica PPK CHORA ...   \n",
       "3    quem viu aquela lutadora modela barbuda tatuad...   \n",
       "4    T√¥ passada com esse cara.... quanta merda pode...   \n",
       "..                                                 ...   \n",
       "495  #VideoShowAoVivo Kkkk ahhh eu morro com esse V...   \n",
       "496  At√© o Padre F√°bio de Melo cantando Trem Bala, ...   \n",
       "497  #MaisVoc√™ minha irm√£ cal√ßa 34 e compra sapato ...   \n",
       "498                 Iris t√£o fofa e linda üòªüòªüíô #EDeCasa   \n",
       "499  #MaisVoce O FGTS n√£o √© descontado do funcion√°r...   \n",
       "\n",
       "                             tweets_translater_unicamp  \\\n",
       "0    The #encontro Program was showing a family tha...   \n",
       "1                                        I loved them.   \n",
       "2          CATRA launching its new song PPK CHORA no k   \n",
       "3                                        #MasterChefBR   \n",
       "4                I'm handled with this guy.... how mu\"   \n",
       "..                                                 ...   \n",
       "495                                   I love this guy!   \n",
       "496  Even Father F√°bio de Melo singing Trem Bala, i...   \n",
       "497  Never had difficulty in finding this number of...   \n",
       "498          Iris so fool and beautiful Iris  #EDeCasa   \n",
       "499  The obligation is only of the company, which must   \n",
       "\n",
       "                                tweets_translater_face  \n",
       "0    What a beautiful thing! The #touch program was...  \n",
       "1    For more I met with the Galvan Sisters, I love...  \n",
       "2    Mr. CATRA releases his new song PPK CHORA on k...  \n",
       "3    Who saw that fighter model tattooed? #MasterCh...  \n",
       "4    How much shit can come out of someone‚Äôs mouth ...  \n",
       "..                                                 ...  \n",
       "495  #VideoShowAoLive Kkkk ahhh I die with this Sum...  \n",
       "496  Even the Fabius Father of Melo singing Trem Ba...  \n",
       "497  You my sister shoes 34 and buy shoes in any st...  \n",
       "498           Iris so foolish and beautiful üèª #EDeCasa  \n",
       "499  #MaisVoce The FGTS is not discounted from the ...  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fd9cef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m classificacoes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigem_portugues\u001b[39m\u001b[38;5;124m'\u001b[39m: df_portuguese[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m500\u001b[39m], \n\u001b[1;32m----> 2\u001b[0m                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifica√ß√£o_hug_com_unicamp_translater\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweets_translater_unicamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifierD\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      3\u001b[0m                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifica√ß√£o_rabin_com_unicamp_translater\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets_translater_unicamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(new_classifier),\n\u001b[0;32m      4\u001b[0m                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifica√ß√£o_hug_com_face_translater\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets_translater_face\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(classifierD),\n\u001b[0;32m      5\u001b[0m                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifica√ß√£o_rabin_com_face_translater\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets_translater_face\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(new_classifier)})\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m   4254\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4255\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4355\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mclassifierD\u001b[1;34m(frase)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassifierD\u001b[39m(frase):    \n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrase\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:125\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m        If `self.return_all_scores=True`, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;66;03m# This pipeline is odd, and return a list when single item is run\u001b[39;00m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [result]\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1026\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1032\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1032\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m   1033\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1034\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:134\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[1;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, GenericTensor]:\n\u001b[0;32m    133\u001b[0m     return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(inputs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n",
      "File \u001b[1;32mc:\\users\\linco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2426\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2432\u001b[0m     )\n\u001b[0;32m   2434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2438\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "classificacoes = pd.DataFrame(data = {'origem_portugues': df_portuguese['sentiment'][:500], \n",
    "                                      'classifica√ß√£o_hug_com_unicamp_translater': dataset['tweets_translater_unicamp'].apply(classifierD),\n",
    "                                      'classifica√ß√£o_rabin_com_unicamp_translater': dataset['tweets_translater_unicamp'].apply(new_classifier),\n",
    "                                      'classifica√ß√£o_hug_com_face_translater': dataset['tweets_translater_face'].apply(classifierD),\n",
    "                                      'classifica√ß√£o_rabin_com_face_translater': dataset['tweets_translater_face'].apply(new_classifier)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e72e8476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9786376357078552}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(Tweets_in_English['tweets_translater_unicamp'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0eb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacoes['classifica√ß√£o_hug'] = classificacoes['classifica√ß√£o_hug'].apply(lambda x: x[0]['label'])\n",
    "classificacoes['classifica√ß√£o_hug_face'] = classificacoes['classifica√ß√£o_hug_face'].apply(lambda x: x[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacoes['classifica√ß√£o_hug'] = classificacoes['classifica√ß√£o_hug'].replace('POSITIVE', 1).replace('NEGATIVE',-1)\n",
    "classificacoes['classifica√ß√£o_hug_face'] = classificacoes['classifica√ß√£o_hug_face'].replace('POSITIVE', 1).replace('NEGATIVE',-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2284de",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacoes['classifica√ß√£o_rabin_hug'] = classificacoes['classifica√ß√£o_rabin_hug'].replace(0, -1).replace(1,0).replace(2,1)\n",
    "classificacoes['classifica√ß√£o_rabin_face'] = classificacoes['classifica√ß√£o_rabin_face'].replace(0, -1).replace(1,0).replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_in_English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificacoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655aa45e",
   "metadata": {},
   "source": [
    "## Acuracia da origem com classificador do huggingface e traduzido com o tradutor da unicamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fafae750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soma = 0\n",
    "for i in range (len(classificacoes)):\n",
    "    if(int(classificacoes['origem_portugues'][i]) == int(classificacoes['classifica√ß√£o_uni'][i])):\n",
    "        soma+=1\n",
    "\n",
    "soma/len(classificacoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680efd9",
   "metadata": {},
   "source": [
    "## Acuracia da origem com classificador do rabin e traduzido com o tradutor da unicamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c28cf363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soma = 0\n",
    "for i in range (len(classificacoes)):\n",
    "    if(int(classificacoes['origem_portugues'][i]) == int(classificacoes['classifica√ß√£o_rabin_uni'][i])):\n",
    "        soma+=1\n",
    "\n",
    "soma/len(classificacoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5041356",
   "metadata": {},
   "source": [
    "## Acuracia da origem com classificador do huggingface e traduzido com o tradutor da m2m100 ou face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "483493dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soma = 0\n",
    "for i in range (len(classificacoes)):\n",
    "    if(int(classificacoes['origem_portugues'][i]) == int(classificacoes['classifica√ß√£o_uni_face'][i])):\n",
    "        soma+=1\n",
    "\n",
    "soma/len(classificacoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba6a2a",
   "metadata": {},
   "source": [
    "## Acuracia da origem com classificador do rabin e traduzido com o tradutor da m2m100 ou face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "76830d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soma = 0\n",
    "for i in range (len(classificacoes)):\n",
    "    if(int(classificacoes['origem_portugues'][i]) == int(classificacoes['classifica√ß√£o_rabin_face'][i])):\n",
    "        soma+=1\n",
    "\n",
    "soma/len(classificacoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec282b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
